{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost as xbg\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def testing_grounds(X_train2, y_train2, X_test2, y_test2, model_options, model_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that takes in multiple parawwmeters for cleaning type, vectorizer, model, and hyperparameter tuning and tests all possible combinations.\n",
    "    \n",
    "   \n",
    "    \n",
    "    :param model_options: a dictionary of models to test.\n",
    "    :Example: \n",
    "    {'logistic_regression':LogisticRegression(), 'multinomial_nb':MultinomialNB()}\n",
    "    \n",
    "    :param model_params: a dictionary of model parameters to test.\n",
    "    :Example: {'model_name':{ 'param_name':[param options] }}\n",
    "    \n",
    "    \n",
    "    :return: dictionary 2 levels deep with all passed options as keys and the best parameters and scores for each combination.\n",
    "    :Example: best_runs[model_options]['score','params']\n",
    "    \"\"\"\n",
    "    num_grids = len(model_options)\n",
    "    print(\"Total Number of Gridsearches:\", num_grids)\n",
    "    index=1\n",
    "    best_runs = defaultdict(dict)\n",
    "    # [stem/lem/none] [logistic/multinomial] [count/tfidf] [score/params]\n",
    "\n",
    "\n",
    "\n",
    "   # X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "\n",
    "    #testing the models\n",
    "    for model_k, model_v in model_options.items():\n",
    "\n",
    "\n",
    "        parameters = {}\n",
    "\n",
    "        for params_k, params_v in model_params.items():\n",
    "            if params_k == model_k:\n",
    "                for param_k, param_v in params_v.items():\n",
    "                    parameters['model'] = param_v #model__'+param_k\n",
    "\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('model', model_v)\n",
    "        ])\n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=3)\n",
    "\n",
    "        print(f\"Performing grid search #{index} of {num_grids}...\")\n",
    "        index+=1\n",
    "        print(f\"Pipeline: {model_k}\\n\")\n",
    "        print(\"Parameters:\")\n",
    "        display(parameters)\n",
    "        t0 = time.time()\n",
    "        grid_search.fit(X_train2, y_train2)\n",
    "        print(\"Done in %0.3fs\" % (time.time() - t0))\n",
    "        print()\n",
    "\n",
    "        print(\"Best train score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best test score: %0.3f\" % grid_search.score(X_test2, y_test2))\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        best_runs[model_k]['score_train'] = grid_search.best_score_\n",
    "        best_runs[model_k]['params'] = best_parameters\n",
    "        best_runs[model_k]['model'] = grid_search.best_estimator_\n",
    "        best_runs[model_k]['score_test'] = grid_search.score(X_test2, y_test2)\n",
    "#         for param_name in sorted(parameters.keys()):\n",
    "#             print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "#             best_runs[model_k]['params'] = best_parameters\n",
    "            \n",
    "        print('\\n\\n')\n",
    "    \n",
    "    return best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def best_model(best_runs):\n",
    "    best_train_score = -sys.maxsize + 10\n",
    "    best_test_score = -sys.maxsize + 10\n",
    "    best_train_model = ''\n",
    "    best_test_model = ''\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    for model_k, model_v in best_runs.items():\n",
    "\n",
    "        if model_v['score_train'] > best_train_score:\n",
    "            best_train_score = model_v['score_train']\n",
    "            best_train_model = model_k\n",
    "            \n",
    "        if model_v['score_test'] > best_test_score:\n",
    "            best_test_score = model_v['score_test']\n",
    "            best_test_model = model_k\n",
    "\n",
    "    print(f'Best Train Score:  {best_train_score} from model {best_train_model}')\n",
    "    print(f'Best Test Score:  {best_test_score} from model {best_test_model}')\n",
    "    print('Best Model:      ')\n",
    "    display(best_model)\n",
    "    print()\n",
    "    print('Best Params:     ')\n",
    "    display(best_params)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "train = pd.read_csv('../assets/train_cleaned.csv')\n",
    "test = pd.read_csv('../assets/test_cleaned.csv')\n",
    "sample = pd.read_csv('../assets/sampleSubmission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train split the train off of date, multiple years' input as train and test is the last year (2013?)\n",
    "#How can I test the values I get back? the labels!\n",
    "mask = train['year']==2013\n",
    "X_test = train[mask]\n",
    "X_train = train[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2013])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2007, 2009, 2011])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Labels\n",
    "labels_entire = train.WnvPresent.values\n",
    "labels_train = X_train.WnvPresent.values\n",
    "labels_test = X_test.WnvPresent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop address columns, wnv present, num mosquitos, and year\n",
    "train = train.drop(['WnvPresent', 'NumMosquitos', 'year'], axis = 1)\n",
    "X_train = X_train.drop(['WnvPresent', 'NumMosquitos', 'year'], axis = 1)\n",
    "X_test = X_test.drop(['WnvPresent', 'NumMosquitos', 'year'], axis = 1)\n",
    "\n",
    "\n",
    "train = train.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "X_train = X_train.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "X_test = X_test.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "\n",
    "test = test.drop([ 'Address', 'AddressNumberAndStreet'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Test - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a Random Forest Classifier \n",
    "clf = ensemble.RandomForestClassifier(n_jobs=-1, n_estimators=1000, min_samples_split=2)\n",
    "clf.fit(X_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000836120401338"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Gridsearches: 5\n",
      "Performing grid search #1 of 5...\n",
      "Pipeline: logistic_regression\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:   37.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 86.688s\n",
      "\n",
      "Best train score: 0.897\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #2 of 5...\n",
      "Pipeline: random_forest_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 8.959s\n",
      "\n",
      "Best train score: 0.607\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #3 of 5...\n",
      "Pipeline: extra_trees_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 6.701s\n",
      "\n",
      "Best train score: 0.728\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #4 of 5...\n",
      "Pipeline: ada_boost_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 9.780s\n",
      "\n",
      "Best train score: 0.586\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #5 of 5...\n",
      "Pipeline: gradient_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.150s\n",
      "\n",
      "Best train score: 0.487\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing a bunch of models in the background while I try to feature engineer. No real hyper-parameter tuning\n",
    "n_est = [100, 500, 1000, 1500]\n",
    "\n",
    "\n",
    "model_options = {\n",
    "    'logistic_regression':LogisticRegressionCV(random_state=42, solver='sag', max_iter = 10000),\n",
    "    \n",
    "    #The regressors did not do as well as the classifiers\n",
    "    \n",
    "    'linreg':LinearRegression(),\n",
    "    'xgboost':xbg.XGBRegressor(),\n",
    "    'random_forest_regressor':RandomForestRegressor(n_estimators=n_est),\n",
    "    'extra_trees_regressor':ExtraTreesRegressor(n_estimators=n_est),\n",
    "    'ada_boost_regressor':AdaBoostRegressor(n_estimators=n_est),\n",
    "    'gradient_regressor':GradientBoostingRegressor(n_estimators=n_est),\n",
    "   \n",
    "    'random_forest_classifier':RandomForestClassifier(n_estimators=n_est),\n",
    "    'extra_trees_classifier':ExtraTreesClassifier(n_estimators=n_est),\n",
    "    'ada_boost_classifier':AdaBoostClassifier(n_estimators=n_est),\n",
    "    'gradient_classifier':GradientBoostingClassifier()\n",
    "    \n",
    "}\n",
    "model_params = {\n",
    "    'linreg':{\n",
    "        \n",
    "    },\n",
    "    'xgboost':{\n",
    "        \n",
    "    },\n",
    "    'random_forest_regressor':{\n",
    "        \n",
    "    },\n",
    "    'extra_trees_regressor':{\n",
    "        \n",
    "    },\n",
    "    'ada_boost_regressor':{\n",
    "        \n",
    "    },\n",
    "    'gradient_regressor':{\n",
    "        \n",
    "    },\n",
    "   \n",
    "    'random_forest_classifier':{\n",
    "        \n",
    "    },\n",
    "    'extra_trees_classifier':{\n",
    "        \n",
    "    },\n",
    "    'ada_boost_classifier':{\n",
    "        \n",
    "    },\n",
    "    'gradient_classifier':{\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "best_runs = testing_grounds(X_train, labels_train, X_test, labels_test, model_options, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression\n",
      "    Train Score: 0.8969682031057432\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: random_forest_classifier\n",
      "    Train Score: 0.6070988415085038\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: extra_trees_classifier\n",
      "    Train Score: 0.7283707172787774\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: ada_boost_classifier\n",
      "    Train Score: 0.5860241557801331\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: gradient_classifier\n",
      "    Train Score: 0.4868129159477446\n",
      "    Test Score: 0.9000836120401338\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best Train Score:  0.8969682031057432 from model logistic_regression\n",
      "Best Test Score:  0.9000836120401338 from model logistic_regression\n",
      "Best Model:      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params:     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in best_runs.items():\n",
    "    print(f'Model: {k}')\n",
    "    print(f'    Train Score: {v[\"score_train\"]}')\n",
    "    print(f'    Test Score: {v[\"score_test\"]}')\n",
    "    \n",
    "print('\\n\\n\\n\\n\\n')\n",
    "best_model(best_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000836120401338"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Highest scoring model in the function\n",
    "logreg = LogisticRegressionCV(random_state=42, solver='sag', max_iter = 10000)\n",
    "logreg.fit(X_train, labels_train)\n",
    "logreg.score(X_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000836120401338"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrg2 = best_runs['logistic_regression']['model']\n",
    "logrg2.fit(X_train, labels_train)\n",
    "logrg2.score(X_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9615479418289377"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrg2.score(X_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# These two cells are for the test submission to kaggle\n",
    "\n",
    "# Random Forest Classifier \n",
    "clf_final = ensemble.RandomForestClassifier(n_jobs=-1, n_estimators=1000, min_samples_split=2)\n",
    "clf_final.fit(train, labels_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to try submitting all of the models' predictions?\n",
    "for model_k, model_v in best_runs.items():\n",
    "    predictions = model_v['model'].predict_proba(test.drop(columns='Id'))[:,1]\n",
    "    sample['WnvPresent'] = predictions\n",
    "    sample.to_csv('../assets/submission_all_'+model_k'.csv', index=False)\n",
    "\n",
    "predictions = logrg2.predict_proba(test.drop(columns='Id'))[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv('../assets/submission_logrg2.csv', index=False)\n",
    "    \n",
    "predictions = clf_final.predict_proba(test.drop(columns='Id'))[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv('../assets/submission_clf_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1], test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions and submission file\n",
    "predictions = clf_final.predict_proba(test.drop(columns='Id'))[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv('../assets/submission_4_randomforest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = test.merge(sample, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = result_df['WnvPresent'].min()\n",
    "maximum = result_df['WnvPresent'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = ((maximum - minimum)/2) + minimum\n",
    "new_results = result_df[result_df['WnvPresent']>=midpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results=new_results.sort_values('WnvPresent', ascending=False)\n",
    "new_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualization of ALL the areas where we predicted we should spray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "mapdata = np.loadtxt(\"../assets/mapdata_copyright_openstreetmap_contributors.txt\")\n",
    "traps = result_df[['Trap','Longitude', 'Latitude', 'WnvPresent']]\n",
    "\n",
    "# these are needed for plotting densities over map image,\n",
    "# it changes alpha channel?\n",
    "\n",
    "# see,\n",
    "# Meaning of the colormap._lut list in matplotlib.color\n",
    "# http://stackoverflow.com/questions/18035411/meaning-of-the-colormap-lut-list-in-matplotlib-color\n",
    "\n",
    "alpha_cm = plt.cm.Reds\n",
    "alpha_cm._init()\n",
    "alpha_cm._lut[:-3,-1] = abs(np.logspace(0, 1, alpha_cm.N) / 10 - 1)[::-1]\n",
    "\n",
    "\n",
    "aspect = mapdata.shape[0] * 1.0 / mapdata.shape[1]\n",
    "lon_lat_box = (-88, -87.5, 41.6, 42.1) # xmin, xmax, ymin, ymax\n",
    "\n",
    "sigthings = traps[traps['WnvPresent'] > 0]\n",
    "sigthings = sigthings.groupby(['Trap','Longitude', 'Latitude']).max()['WnvPresent'].reset_index()\n",
    "X = sigthings[['Longitude', 'Latitude']].values\n",
    "kd = KernelDensity(bandwidth=0.02)\n",
    "kd.fit(X)\n",
    "\n",
    "xv,yv = np.meshgrid(np.linspace(-88, -87.5, 100), np.linspace(41.6, 42.1, 100))\n",
    "gridpoints = np.array([xv.ravel(),yv.ravel()]).T\n",
    "zv = np.exp(kd.score_samples(gridpoints).reshape(100,100))\n",
    "plt.figure(figsize=(10,14))\n",
    "plt.imshow(mapdata, \n",
    "           cmap=plt.get_cmap('gray'), \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "plt.imshow(zv, \n",
    "           origin='lower', \n",
    "           cmap=alpha_cm, \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "\n",
    "# -> how to use 'extent' in matplotlib.pyplot.imshow\n",
    "# http://stackoverflow.com/questions/6999621/how-to-use-extent-in-matplotlib-pyplot-imshow\n",
    "\n",
    "locations = traps[['Longitude', 'Latitude']].drop_duplicates().values\n",
    "plt.scatter(locations[:,0], locations[:,1], marker='x')\n",
    "\n",
    "plt.savefig('total_predictions_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of the TOP 50% of areas where we predicted we should spray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "mapdata = np.loadtxt(\"../assets/mapdata_copyright_openstreetmap_contributors.txt\")\n",
    "traps = new_results[['Trap','Longitude', 'Latitude', 'WnvPresent']]\n",
    "\n",
    "# these are needed for plotting densities over map image,\n",
    "# it changes alpha channel?\n",
    "\n",
    "# see,\n",
    "# Meaning of the colormap._lut list in matplotlib.color\n",
    "# http://stackoverflow.com/questions/18035411/meaning-of-the-colormap-lut-list-in-matplotlib-color\n",
    "\n",
    "alpha_cm = plt.cm.Reds\n",
    "alpha_cm._init()\n",
    "alpha_cm._lut[:-3,-1] = abs(np.logspace(0, 1, alpha_cm.N) / 10 - 1)[::-1]\n",
    "\n",
    "\n",
    "aspect = mapdata.shape[0] * 1.0 / mapdata.shape[1]\n",
    "lon_lat_box = (-88, -87.5, 41.6, 42.1) # xmin, xmax, ymin, ymax\n",
    "\n",
    "sigthings = traps[traps['WnvPresent'] > 0]\n",
    "sigthings = sigthings.groupby(['Trap','Longitude', 'Latitude']).max()['WnvPresent'].reset_index()\n",
    "X = sigthings[['Longitude', 'Latitude']].values\n",
    "kd = KernelDensity(bandwidth=0.02)\n",
    "kd.fit(X)\n",
    "\n",
    "xv,yv = np.meshgrid(np.linspace(-88, -87.5, 100), np.linspace(41.6, 42.1, 100))\n",
    "gridpoints = np.array([xv.ravel(),yv.ravel()]).T\n",
    "zv = np.exp(kd.score_samples(gridpoints).reshape(100,100))\n",
    "plt.figure(figsize=(10,14))\n",
    "plt.imshow(mapdata, \n",
    "           cmap=plt.get_cmap('gray'), \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "plt.imshow(zv, \n",
    "           origin='lower', \n",
    "           cmap=alpha_cm, \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "\n",
    "# -> how to use 'extent' in matplotlib.pyplot.imshow\n",
    "# http://stackoverflow.com/questions/6999621/how-to-use-extent-in-matplotlib-pyplot-imshow\n",
    "\n",
    "locations = traps[['Longitude', 'Latitude']].drop_duplicates().values\n",
    "plt.scatter(locations[:,0], locations[:,1], marker='x')\n",
    "\n",
    "plt.savefig('top_50_percent_predictions_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
