{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost as xbg\n",
    "from sklearn.linear_model import LogisticRegressionCV, LinearRegression\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def testing_grounds(X_train2, y_train2, X_test2, y_test2, model_options, model_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that takes in multiple parawwmeters for cleaning type, vectorizer, model, and hyperparameter tuning and tests all possible combinations.\n",
    "    \n",
    "   \n",
    "    \n",
    "    :param model_options: a dictionary of models to test.\n",
    "    :Example: \n",
    "    {'logistic_regression':LogisticRegression(), 'multinomial_nb':MultinomialNB()}\n",
    "    \n",
    "    :param model_params: a dictionary of model parameters to test.\n",
    "    :Example: {'model_name':{ 'param_name':[param options] }}\n",
    "    \n",
    "    \n",
    "    :return: dictionary 2 levels deep with all passed options as keys and the best parameters and scores for each combination.\n",
    "    :Example: best_runs[model_options]['score','params']\n",
    "    \"\"\"\n",
    "    num_grids = len(model_options)\n",
    "    print(\"Total Number of Gridsearches:\", num_grids)\n",
    "    index=1\n",
    "    best_runs = defaultdict(dict)\n",
    "    # [stem/lem/none] [logistic/multinomial] [count/tfidf] [score/params]\n",
    "\n",
    "\n",
    "\n",
    "   # X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "\n",
    "    #testing the models\n",
    "    for model_k, model_v in model_options.items():\n",
    "\n",
    "\n",
    "        parameters = {}\n",
    "\n",
    "        for params_k, params_v in model_params.items():\n",
    "            if params_k == model_k:\n",
    "                for param_k, param_v in params_v.items():\n",
    "                    parameters['model__'+param_k] = param_v\n",
    "\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('model', model_v)\n",
    "        ])\n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=3)\n",
    "\n",
    "        print(f\"Performing grid search #{index} of {num_grids}...\")\n",
    "        index+=1\n",
    "        print(f\"Pipeline: {model_k}\\n\")\n",
    "        print(\"Parameters:\")\n",
    "        display(parameters)\n",
    "        t0 = time.time()\n",
    "        grid_search.fit(X_train2, y_train2)\n",
    "        print(\"Done in %0.3fs\" % (time.time() - t0))\n",
    "        print()\n",
    "\n",
    "        print(\"Best train score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best test score: %0.3f\" % grid_search.score(X_test2, y_test2))\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        display(best_parameters)\n",
    "        best_runs[model_k]['score_train'] = grid_search.best_score_\n",
    "        best_runs[model_k]['params'] = best_parameters\n",
    "        best_runs[model_k]['model'] = grid_search.best_estimator_\n",
    "        best_runs[model_k]['score_test'] = grid_search.score(X_test2, y_test2)\n",
    "#         for param_name in sorted(parameters.keys()):\n",
    "#             print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "#             best_runs[model_k]['params'] = best_parameters\n",
    "            \n",
    "        print('\\n\\n')\n",
    "    \n",
    "    return best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def best_model(best_runs):\n",
    "    best_train_score = -sys.maxsize + 10\n",
    "    best_test_score = -sys.maxsize + 10\n",
    "    best_train_model = ''\n",
    "    best_test_model = ''\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    for model_k, model_v in best_runs.items():\n",
    "\n",
    "        if model_v['score_train'] > best_train_score:\n",
    "            best_train_score = model_v['score_train']\n",
    "            best_train_model = model_k\n",
    "            \n",
    "        if model_v['score_test'] > best_test_score:\n",
    "            best_test_score = model_v['score_test']\n",
    "            best_test_model = model_k\n",
    "\n",
    "    print(f'Best Train Score:  {best_train_score} from model {best_train_model}')\n",
    "    print(f'Best Test Score:  {best_test_score} from model {best_test_model}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "train = pd.read_csv('../assets/train_cleaned.csv')\n",
    "test = pd.read_csv('../assets/test_cleaned.csv')\n",
    "sample = pd.read_csv('../assets/sampleSubmission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train split the train off of date, multiple years' input as train and test is the last year (2013?)\n",
    "#How can I test the values I get back? the labels!\n",
    "mask = train['year']==2013\n",
    "X_test = train[mask]\n",
    "X_train = train[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2013])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2007, 2009, 2011])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Labels\n",
    "labels_entire = train.WnvPresent.values\n",
    "labels_train = X_train.WnvPresent.values\n",
    "labels_test = X_test.WnvPresent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop address columns, wnv present, num mosquitos\n",
    "\n",
    "# \n",
    "# train = train.drop(['year'], axis = 1)\n",
    "# X_train = X_train.drop(['year'], axis = 1)\n",
    "# X_test = X_test.drop(['year'], axis = 1)\n",
    "# test = test.drop(['year'], axis = 1)\n",
    "\n",
    "train = train.drop(['WnvPresent', 'NumMosquitos'], axis = 1)\n",
    "X_train = X_train.drop(['WnvPresent', 'NumMosquitos'], axis = 1)\n",
    "X_test = X_test.drop(['WnvPresent', 'NumMosquitos'], axis = 1)\n",
    "\n",
    "\n",
    "train = train.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "X_train = X_train.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "X_test = X_test.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "\n",
    "test = test.drop([ 'Address', 'AddressNumberAndStreet'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Test - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a Random Forest Classifier \n",
    "clf = ensemble.RandomForestClassifier(n_jobs=-1, n_estimators=1000, min_samples_split=2)\n",
    "clf.fit(X_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000836120401338"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9000836120401338 == 0.9000836120401338\n",
    "\n",
    "#I don't really know why all of the scores are all the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# testing a bunch of models in the background while I try to feature engineer. \n",
    "\n",
    "# Split up so if there are any errors it doesn't break all the rest of the grid searching attempt. \n",
    "\n",
    "n_est = [100, 500, 1000, 1500]\n",
    "\n",
    "\n",
    "model_options = {\n",
    "    \n",
    "    \n",
    "    #The regressors did not do as well as the classifiers\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "model_params = {\n",
    "    'linreg':{\n",
    "        \n",
    "    },\n",
    "    'xgboost_regressor':{\n",
    "        'max_depth':[3,50,100],\n",
    "        'learning_rate':[0.1, 0.2, 0.3, 0.4],\n",
    "        'n_estimators':n_est,\n",
    "        \n",
    "    },\n",
    "    'random_forest_regressor':{\n",
    "        'n_estimators':n_est,\n",
    "        'criterion':['mse', 'mae'],\n",
    "        \n",
    "    },\n",
    "    'extra_trees_regressor':{\n",
    "        'n_estimators':n_est,\n",
    "        'criterion':['mse', 'mae'],\n",
    "        \n",
    "    },\n",
    "    'ada_boost_regressor':{\n",
    "        'n_estimators':n_est,\n",
    "        'learning_rate':[0.8, 0.9, 1., 1.1],\n",
    "        'loss':['linear', 'square', 'exponential'],\n",
    "        \n",
    "    },\n",
    "    'gradient_regressor':{\n",
    "        'n_estimators':n_est,\n",
    "        'loss':['ls', 'lad', 'huber', 'quantile'],\n",
    "        'learning_rate':[0.1, 0.2, 0.3],\n",
    "        'subsample':[1.0, ],\n",
    "        'max_depth':[3, 10, 50, 100],\n",
    "        'alpha':[0.9, .8, .7],\n",
    "        \n",
    "    },\n",
    "   \n",
    "    'logistic_regression':{\n",
    "        'solver':['sag', ],\n",
    "        'max_iter':[1000, 100, ],\n",
    "        \n",
    "    },\n",
    "    'random_forest_classifier':{\n",
    "        'n_estimators':n_est,\n",
    "        'criterion':['gini', 'entropy'],\n",
    "        \n",
    "    },\n",
    "    'extra_trees_classifier':{\n",
    "        'n_estimators':n_est,\n",
    "        'criterion':['gini', 'entropy'],\n",
    "        \n",
    "    },\n",
    "    'ada_boost_classifier':{\n",
    "        'n_estimators':n_est,\n",
    "        'learning_rate':[0.8, 0.9, 1., 1.1],\n",
    "        \n",
    "    },\n",
    "    'gradient_classifier':{\n",
    "        'n_estimators':n_est,\n",
    "        'loss':['deviance', 'exponential'],\n",
    "        'learning_rate':[0.8, 0.9, 1., 1.1],\n",
    "        'max_depth':[3, 10, 50, 100],\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "best_runs_all = []\n",
    "print('run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor\n",
    "# linreg                  - negative score\n",
    "# random forest regresson - 1h to run, .03\n",
    "# extra trees regression  - cut\n",
    "\n",
    "model_options = {\n",
    "    # [0:3]\n",
    "    'linreg':LinearRegression(), #negative score\n",
    "    'random_forest_regressor':RandomForestRegressor(random_state=42), #1 hour, .034 test\n",
    "    'extra_trees_regressor':ExtraTreesRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# best_runs_1 = testing_grounds(X_train, labels_train, X_test, labels_test, model_options, model_params)\n",
    "# best_runs_all.extend(best_runs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor\n",
    "# ada boost regressor - did poorly\n",
    "# gradient regressor  - did poorly\n",
    "# xgboost regressor   - did poorly\n",
    "\n",
    "\n",
    "model_options = {\n",
    "    # [3:6]\n",
    "     'ada_boost_regressor':AdaBoostRegressor(random_state=42),\n",
    "     'gradient_regressor':GradientBoostingRegressor(random_state=42),\n",
    "     'xgboost_regessor':xbg.XGBRegressor(random_state=42),\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#best_runs_2 = testing_grounds(X_train, labels_train, X_test, labels_test, model_options, model_params)\n",
    "#best_runs_all.extend(best_runs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Gridsearches: 3\n",
      "Performing grid search #1 of 3...\n",
      "Pipeline: logistic_regression\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__solver': ['sag'], 'model__max_iter': [1000, 100]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=3)]: Done   6 out of   6 | elapsed:   26.3s finished\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 31.542s\n",
      "\n",
      "Best train score: 0.962\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #2 of 3...\n",
      "Pipeline: random_forest_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [100, 500, 1000, 1500],\n",
       " 'model__criterion': ['gini', 'entropy']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  24 out of  24 | elapsed:   23.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 28.379s\n",
      "\n",
      "Best train score: 0.574\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #3 of 3...\n",
      "Pipeline: extra_trees_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [100, 500, 1000, 1500],\n",
       " 'model__criterion': ['gini', 'entropy']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  24 out of  24 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 17.875s\n",
      "\n",
      "Best train score: 0.693\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier \n",
    "\n",
    "# logistic regression      - \n",
    "# random forest classifier - \n",
    "# extra trees classifier   - \n",
    "\n",
    "model_options = {\n",
    "    # [6:9]\n",
    "     'logistic_regression':LogisticRegressionCV(random_state=42),\n",
    "     'random_forest_classifier':RandomForestClassifier(random_state=42),\n",
    "     'extra_trees_classifier':ExtraTreesClassifier(random_state=42),\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "best_runs_3 = testing_grounds(X_train, labels_train, X_test, labels_test, model_options, model_params)\n",
    "best_runs_all.extend(best_runs_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Gridsearches: 3\n",
      "Performing grid search #1 of 3...\n",
      "Pipeline: ada_boost_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [100, 500, 1000, 1500],\n",
       " 'model__learning_rate': [0.8, 0.9, 1.0, 1.1]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  48 out of  48 | elapsed:   53.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 54.124s\n",
      "\n",
      "Best train score: 0.641\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #2 of 3...\n",
      "Pipeline: gradient_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [100, 500, 1000, 1500],\n",
       " 'model__loss': ['deviance', 'exponential'],\n",
       " 'model__learning_rate': [0.8, 0.9, 1.0, 1.1],\n",
       " 'model__max_depth': [3, 10, 50, 100]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 384 out of 384 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 137.110s\n",
      "\n",
      "Best train score: 0.525\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #3 of 3...\n",
      "Pipeline: xgboost_classifier\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.059s\n",
      "\n",
      "Best train score: 0.535\n",
      "Best test score: 0.900\n",
      "Best parameters set:\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Classifier \n",
    "\n",
    "# Ada boost classifier - train .641  test .9\n",
    "# Gradient classifier  - train .  test .9\n",
    "# xgboost classifier   - train .  test .9\n",
    "\n",
    "model_options = {\n",
    "    # [9:12]\n",
    "     'ada_boost_classifier':AdaBoostClassifier(random_state=42),\n",
    "     'gradient_classifier':GradientBoostingClassifier(random_state=42),\n",
    "     'xgboost_classifier':xbg.XGBClassifier(random_state=42),\n",
    "    \n",
    "}\n",
    "\n",
    "best_runs_4 = testing_grounds(X_train, labels_train, X_test, labels_test, model_options, model_params)\n",
    "best_runs_all.extend(best_runs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression\n",
      "    Train Score: 0.9615479418289377\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: random_forest_classifier\n",
      "    Train Score: 0.5738230219373922\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: extra_trees_classifier\n",
      "    Train Score: 0.6925067784076904\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: ada_boost_classifier\n",
      "    Train Score: 0.6407443924081834\n",
      "    Test Score: 0.9000836120401338\n",
      "Model: gradient_classifier\n",
      "    Train Score: 0.5245255114616711\n",
      "    Test Score: 0.8996655518394648\n",
      "Model: xgboost_classifier\n",
      "    Train Score: 0.534631501109194\n",
      "    Test Score: 0.9000836120401338\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best Train Score:  0.9615479418289377 from model logistic_regression\n",
      "Best Test Score:  0.9000836120401338 from model logistic_regression\n",
      "Best Model:      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params:     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_runs_all = {}\n",
    "\n",
    "for i in [ best_runs_3, best_runs_4]: #best_runs_1, best_runs_2,\n",
    "    best_runs_all.update(i)\n",
    "\n",
    "for k, v in best_runs_all.items():\n",
    "    print(f'Model: {k}')\n",
    "    print(f'    Train Score: {v[\"score_train\"]}')\n",
    "    print(f'    Test Score: {v[\"score_test\"]}')\n",
    "    \n",
    "print('\\n\\n\\n\\n\\n')\n",
    "best_model(best_runs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000836120401338"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Highest scoring model in the function\n",
    "logreg = LogisticRegressionCV(random_state=42, solver='sag', max_iter = 10000)\n",
    "logreg.fit(X_train, labels_train)\n",
    "logreg.score(X_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9000836120401338"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrg2 = best_runs_all['logistic_regression']['model']\n",
    "logrg2.fit(X_train, labels_train)\n",
    "logrg2.score(X_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9615479418289377"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrg2.score(X_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These two cells are for the test submission to kaggle\n",
    "\n",
    "# Random Forest Classifier \n",
    "clf_final = ensemble.RandomForestClassifier(n_jobs=-1, n_estimators=1000, min_samples_split=2)\n",
    "clf_final.fit(train, labels_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to try submitting all of the models' predictions?\n",
    "for model_k, model_v in best_runs_all.items():\n",
    "    predictions = model_v['model'].predict_proba(test.drop(columns='Id'))[:,1]\n",
    "    sample['WnvPresent'] = predictions\n",
    "    sample.to_csv('../assets/submissions/submission_all_'+model_k+'.csv', index=False)\n",
    "\n",
    "predictions = logrg2.predict_proba(test.drop(columns='Id'))[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv('../assets/submissions/submission_logrg2.csv', index=False)\n",
    "    \n",
    "predictions = clf_final.predict_proba(test.drop(columns='Id'))[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv('../assets/submissions/submission_clf_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1], test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions for best kaggle model - xgboost\n",
    "predictions = .predict_proba(test.drop(columns='Id'))[:,1]\n",
    "sample['WnvPresent'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = test.merge(sample, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = result_df['WnvPresent'].min()\n",
    "maximum = result_df['WnvPresent'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = ((maximum - minimum)/2) + minimum\n",
    "new_results = result_df[result_df['WnvPresent']>=midpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results=new_results.sort_values('WnvPresent', ascending=False)\n",
    "new_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualization of ALL the areas where we predicted we should spray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "mapdata = np.loadtxt(\"../assets/mapdata_copyright_openstreetmap_contributors.txt\")\n",
    "traps = result_df[['Trap','Longitude', 'Latitude', 'WnvPresent']]\n",
    "\n",
    "# these are needed for plotting densities over map image,\n",
    "# it changes alpha channel?\n",
    "\n",
    "# see,\n",
    "# Meaning of the colormap._lut list in matplotlib.color\n",
    "# http://stackoverflow.com/questions/18035411/meaning-of-the-colormap-lut-list-in-matplotlib-color\n",
    "\n",
    "alpha_cm = plt.cm.Reds\n",
    "alpha_cm._init()\n",
    "alpha_cm._lut[:-3,-1] = abs(np.logspace(0, 1, alpha_cm.N) / 10 - 1)[::-1]\n",
    "\n",
    "\n",
    "aspect = mapdata.shape[0] * 1.0 / mapdata.shape[1]\n",
    "lon_lat_box = (-88, -87.5, 41.6, 42.1) # xmin, xmax, ymin, ymax\n",
    "\n",
    "sigthings = traps[traps['WnvPresent'] > 0]\n",
    "sigthings = sigthings.groupby(['Trap','Longitude', 'Latitude']).max()['WnvPresent'].reset_index()\n",
    "X = sigthings[['Longitude', 'Latitude']].values\n",
    "kd = KernelDensity(bandwidth=0.02)\n",
    "kd.fit(X)\n",
    "\n",
    "xv,yv = np.meshgrid(np.linspace(-88, -87.5, 100), np.linspace(41.6, 42.1, 100))\n",
    "gridpoints = np.array([xv.ravel(),yv.ravel()]).T\n",
    "zv = np.exp(kd.score_samples(gridpoints).reshape(100,100))\n",
    "plt.figure(figsize=(10,14))\n",
    "plt.imshow(mapdata, \n",
    "           cmap=plt.get_cmap('gray'), \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "plt.imshow(zv, \n",
    "           origin='lower', \n",
    "           cmap=alpha_cm, \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "\n",
    "# -> how to use 'extent' in matplotlib.pyplot.imshow\n",
    "# http://stackoverflow.com/questions/6999621/how-to-use-extent-in-matplotlib-pyplot-imshow\n",
    "\n",
    "locations = traps[['Longitude', 'Latitude']].drop_duplicates().values\n",
    "plt.scatter(locations[:,0], locations[:,1], marker='x')\n",
    "\n",
    "plt.savefig('total_predictions_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of the TOP 50% of areas where we predicted we should spray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "mapdata = np.loadtxt(\"../assets/mapdata_copyright_openstreetmap_contributors.txt\")\n",
    "traps = new_results[['Trap','Longitude', 'Latitude', 'WnvPresent']]\n",
    "\n",
    "# these are needed for plotting densities over map image,\n",
    "# it changes alpha channel?\n",
    "\n",
    "# see,\n",
    "# Meaning of the colormap._lut list in matplotlib.color\n",
    "# http://stackoverflow.com/questions/18035411/meaning-of-the-colormap-lut-list-in-matplotlib-color\n",
    "\n",
    "alpha_cm = plt.cm.Reds\n",
    "alpha_cm._init()\n",
    "alpha_cm._lut[:-3,-1] = abs(np.logspace(0, 1, alpha_cm.N) / 10 - 1)[::-1]\n",
    "\n",
    "\n",
    "aspect = mapdata.shape[0] * 1.0 / mapdata.shape[1]\n",
    "lon_lat_box = (-88, -87.5, 41.6, 42.1) # xmin, xmax, ymin, ymax\n",
    "\n",
    "sigthings = traps[traps['WnvPresent'] > 0]\n",
    "sigthings = sigthings.groupby(['Trap','Longitude', 'Latitude']).max()['WnvPresent'].reset_index()\n",
    "X = sigthings[['Longitude', 'Latitude']].values\n",
    "kd = KernelDensity(bandwidth=0.02)\n",
    "kd.fit(X)\n",
    "\n",
    "xv,yv = np.meshgrid(np.linspace(-88, -87.5, 100), np.linspace(41.6, 42.1, 100))\n",
    "gridpoints = np.array([xv.ravel(),yv.ravel()]).T\n",
    "zv = np.exp(kd.score_samples(gridpoints).reshape(100,100))\n",
    "plt.figure(figsize=(10,14))\n",
    "plt.imshow(mapdata, \n",
    "           cmap=plt.get_cmap('gray'), \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "plt.imshow(zv, \n",
    "           origin='lower', \n",
    "           cmap=alpha_cm, \n",
    "           extent=lon_lat_box, \n",
    "           aspect=aspect)\n",
    "\n",
    "# -> how to use 'extent' in matplotlib.pyplot.imshow\n",
    "# http://stackoverflow.com/questions/6999621/how-to-use-extent-in-matplotlib-pyplot-imshow\n",
    "\n",
    "locations = traps[['Longitude', 'Latitude']].drop_duplicates().values\n",
    "plt.scatter(locations[:,0], locations[:,1], marker='x')\n",
    "\n",
    "plt.savefig('top_50_percent_predictions_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
